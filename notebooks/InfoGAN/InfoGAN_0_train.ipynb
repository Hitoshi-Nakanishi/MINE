{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InfoGAN\n",
    "referred to [the blog](http://peluigi.hatenablog.com/entry/2018/08/29/120314)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, argparse, sys \n",
    "sys.path.append('..')\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.utils as vutils\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "from InfoGAN.model import Generator, Discriminator, ContStatistics, DiscStatistics\n",
    "\n",
    "batch_size = 100\n",
    "lr = 1e-4\n",
    "latent_size = 256\n",
    "num_epochs = 100\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "output_dir = Path(\"../results/InfoGAN\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALI_BiGAN  MNIST  tv\n"
     ]
    }
   ],
   "source": [
    "!ls ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_LABEL = 10\n",
    "BATCH_SIZE = 100\n",
    "NUM_WORKERS = 8\n",
    "RANGE = 1\n",
    "train_data = MNIST(\"../data/MNIST\", train=True, download=True, transform=ToTensor())\n",
    "loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                    drop_last=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "G = Generator().to(device)\n",
    "D = Discriminator().to(device)\n",
    "DiscS = DiscStatistics().to(device)\n",
    "ContS1 = ContStatistics().to(device)\n",
    "ContS2 = ContStatistics().to(device)\n",
    "optimG = optim.Adam(G.parameters(), lr=1e-3, betas=(0.5, 0.999))\n",
    "optimD = optim.Adam(D.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "optimS = optim.Adam([{\"params\":DiscS.parameters()}, {\"params\":ContS1.parameters()},\n",
    "                     {\"params\":ContS2.parameters()}], lr=1e-3, betas=(0.5, 0.999))\n",
    "label = torch.zeros(BATCH_SIZE).to(device).float()\n",
    "real_label, fake_label = 1, 0\n",
    "\n",
    "c = torch.linspace(-1, 1, 10).repeat(10).reshape(-1, 1)\n",
    "c1 = torch.cat([c, torch.zeros_like(c)], 1).float() * RANGE\n",
    "c2 = torch.cat([torch.zeros_like(c), c], 1).float() * RANGE\n",
    "idx = torch.from_numpy(np.arange(10).repeat(10))\n",
    "one_hot = torch.zeros((BATCH_SIZE, CLASS_LABEL)).float()\n",
    "one_hot[range(BATCH_SIZE), idx] = 1\n",
    "fix_z = torch.Tensor(BATCH_SIZE, 62).uniform_(-1, 1)\n",
    "fix_noise1 = torch.cat([fix_z, c1, one_hot], 1)[...,None,None].to(device)\n",
    "fix_noise2 = torch.cat([fix_z, c2, one_hot], 1)[...,None,None].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [0/100], iter [599/600], D : 0.509, G : 1.59, S : 0.0288\n",
      "epoch [4/100], iter [599/600], D : 1.23, G : 0.88, S : 0.00743\n",
      "epoch [8/100], iter [599/600], D : 1.11, G : 0.877, S : 0.0388\n",
      "epoch [12/100], iter [599/600], D : 1.24, G : 1.13, S : 0.00333\n",
      "epoch [16/100], iter [599/600], D : 1.09, G : 0.994, S : 0.00202\n",
      "epoch [20/100], iter [599/600], D : 1.04, G : 1.16, S : 0.0108\n",
      "epoch [24/100], iter [599/600], D : 1.04, G : 0.962, S : 0.0277\n",
      "epoch [28/100], iter [599/600], D : 1.17, G : 1.37, S : 0.0017\n",
      "epoch [32/100], iter [599/600], D : 0.918, G : 1.26, S : 0.000884\n",
      "epoch [36/100], iter [599/600], D : 1.07, G : 1.48, S : 0.00046\n",
      "epoch [40/100], iter [599/600], D : 0.938, G : 1.18, S : 0.000251\n",
      "epoch [44/100], iter [599/600], D : 1.01, G : 0.816, S : 0.00739\n",
      "epoch [48/100], iter [599/600], D : 0.878, G : 1.11, S : 0.000651\n",
      "epoch [52/100], iter [599/600], D : 0.851, G : 1.04, S : 0.000169\n",
      "epoch [56/100], iter [599/600], D : 0.935, G : 1.13, S : 0.000798\n",
      "epoch [60/100], iter [599/600], D : 0.856, G : 1.49, S : 0.000198\n",
      "epoch [64/100], iter [599/600], D : 0.726, G : 1.85, S : 4.7e-05\n",
      "epoch [68/100], iter [599/600], D : 0.769, G : 1.15, S : 0.000437\n",
      "epoch [72/100], iter [599/600], D : 0.684, G : 1.51, S : 6.24e-05\n",
      "epoch [76/100], iter [599/600], D : 0.655, G : 1.65, S : 8.53e-05\n",
      "epoch [80/100], iter [599/600], D : 0.715, G : 1.45, S : 0.000381\n",
      "epoch [84/100], iter [599/600], D : 0.562, G : 1.76, S : 0.000103\n",
      "epoch [88/100], iter [599/600], D : 0.598, G : 2.03, S : 9.01e-05\n",
      "epoch [92/100], iter [599/600], D : 0.574, G : 1.7, S : 0.0497\n",
      "epoch [96/100], iter [599/600], D : 0.49, G : 1.73, S : 9.29e-05\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    for i, (d_real, _) in enumerate(loader):\n",
    "        optimD.zero_grad()\n",
    "        d_real = d_real.to(device)\n",
    "        label.fill_(real_label)\n",
    "        real_prob = D(d_real).squeeze()\n",
    "        real_loss = F.binary_cross_entropy_with_logits(real_prob, label)\n",
    "        real_loss.backward()\n",
    "\n",
    "        label.fill_(fake_label)\n",
    "        ### get noise\n",
    "        idx = torch.randint(0, 10, (BATCH_SIZE,)).long()\n",
    "        disc_c = torch.eye(10)[idx][...,None,None].float().to(device)\n",
    "        cont_c = torch.zeros(BATCH_SIZE, 2, 1, 1).uniform_(-1, 1).float().to(device) * RANGE\n",
    "        z = torch.zeros(BATCH_SIZE, 62, 1, 1).uniform_(-1, 1).float().to(device)\n",
    "        noise = torch.cat([z, cont_c, disc_c], 1).to(device).float()\n",
    "        ### generate\n",
    "        d_fake = G(noise)\n",
    "        d_fake_series = d_fake.reshape(100, -1)\n",
    "        fake_prob = D(d_fake.detach()).squeeze()\n",
    "        fake_loss = F.binary_cross_entropy_with_logits(fake_prob, label)\n",
    "        fake_loss.backward()\n",
    "        loss_D = real_loss + fake_loss\n",
    "        optimD.step()\n",
    "\n",
    "        # generator\n",
    "        optimG.zero_grad()\n",
    "        optimS.zero_grad()\n",
    "        label.fill_(real_label)\n",
    "        ## adversarial loss\n",
    "        inv_fake_prob = D(d_fake).squeeze()\n",
    "        inv_fake_loss = F.binary_cross_entropy_with_logits(inv_fake_prob, label)\n",
    "        ## MINE\n",
    "        ### c ~ P(C)\n",
    "        idx = torch.randint(0, 10, (100,)).long()\n",
    "        disc_c_bar = torch.eye(10)[idx].float().to(device)\n",
    "        cont_c_bar = torch.zeros(100, 2, 1, 1).uniform_(-1, 1).float().to(device) * RANGE\n",
    "        ### discrete variable\n",
    "        joint_disc = DiscS(torch.cat([d_fake_series, disc_c.reshape(100, -1)], 1))\n",
    "        marginal_disc = DiscS(torch.cat([d_fake_series, disc_c_bar.reshape(100, -1)], 1))\n",
    "        ### continuout variable\n",
    "        joint_cont1 = ContS1(torch.cat([d_fake_series, cont_c[:,0].reshape(100, -1)], 1))\n",
    "        joint_cont2 = ContS2(torch.cat([d_fake_series, cont_c[:,1].reshape(100, -1)], 1))\n",
    "        marginal_cont1 = ContS1(torch.cat([d_fake_series, cont_c_bar[:,0].reshape(100, -1)], 1))\n",
    "        marginal_cont2 = ContS2(torch.cat([d_fake_series, cont_c_bar[:,1].reshape(100, -1)], 1))\n",
    "        ### calc mutual information\n",
    "        mi_disc = F.softplus(-joint_disc).mean() + F.softplus(marginal_disc).mean()\n",
    "        mi_cont1 = F.softplus(-joint_cont1).mean() + F.softplus(marginal_cont1).mean()\n",
    "        mi_cont2 = F.softplus(-joint_cont2).mean() + F.softplus(marginal_cont2).mean()\n",
    "        mi = (mi_disc + mi_cont1 + mi_cont2)/3\n",
    "\n",
    "        loss = mi + inv_fake_loss\n",
    "        loss.backward()\n",
    "        optimG.step()\n",
    "        optimS.step()\n",
    "        if i == 599 and epoch %4 == 0:\n",
    "            print(\"epoch [{}/{}], iter [{}/{}], D : {:.3}, G : {:.3}, S : {:.3}\".format(\n",
    "                epoch, 100, i, len(loader), loss_D.item(), inv_fake_loss.item(), mi.item()\n",
    "            ))\n",
    "    with torch.no_grad():\n",
    "        fake1 = G(fix_noise1)\n",
    "        fake2 = G(fix_noise2)\n",
    "        vutils.save_image(fake1.detach(), \n",
    "                          str(output_dir / Path(f\"{epoch}epoch_fake1.png\")),\n",
    "                          normalize=True, nrow=10)\n",
    "        vutils.save_image(fake2.detach(), \n",
    "                          str(output_dir / Path(f\"{epoch}epoch_fake2.png\")),\n",
    "                          normalize=True, nrow=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
